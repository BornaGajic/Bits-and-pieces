{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://github.com/aamini/introtodeeplearning/\n",
    "- https://github.com/pytorch/examples/tree/master/word_language_model\n",
    "- http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "- https://github.com/karpathy/char-rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/4136/1*SKGAqkVVzT6co-sZ29ze-g.png\" width=\"70%\"> \n",
    "\n",
    "- RNN učimo generirati tekst\n",
    "- slučaj 1: \"_na <span style=\"color:blue\">nebu</span> su <span style=\"color:blue\">oblaci</span>\"\n",
    "- slučaj 2: \"_Rodio sam se u <span style=\"color:blue\">Francuskoj</span>. Tamo sam živio do desete godine. Stoga tečno pričam <span style=\"color:blue\">francuski</span>._\"\n",
    "- razmak između bitnih informacija postaje velik\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-shorttermdepdencies.png\" width=\"60%\"> \n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-longtermdependencies.png\" width=\"60%\"> \n",
    "\n",
    "\n",
    "- LSTM (Long Short Term Memory) su varijanta RNN\n",
    "- imaju mogućnost učiti long-term dependecies\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-SimpleRNN.png\" width=\"60%\"> \n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"60%\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- horizontalna linija koja prenosi informacije\n",
    "- vrata (gates) koja propuštaju ili ne informaciju\n",
    "- vrata su sigmoid + $\\times$\n",
    "- sigmoid funkcija vrati postotak koliko informacije propustiti\n",
    "1. Prva odluka: koju informaciju propustiti iz prošlog stanja: *forget gate*\n",
    "    - $f_t = \\sigma(W_f [h_{t-1},x_t] + b_f)$\n",
    "2. Druga odluka: koju novu informaciju uzeti:\n",
    "    - sigmoid sloj: koje vrijednosti ćemo ažurirati\n",
    "    - $i_t = \\sigma(W_i[h_{t-1},x_t] + b_i)$\n",
    "    - tanh sloj: kreira novi vektor koji je kandidat za sljedeće stanje\n",
    "    - $\\hat{C}_t = \\tanh{(W_c[h_{t-1},x_t]+ b_c)}$\n",
    "3. Ažuriranje stanja $C_{t-1} \\rightarrow C_t$\n",
    "    - $C_t = f_t\\cdot C_{t-1} + i_t\\cdot  \\hat{C}_t$\n",
    "4. Odluka o izlazu\n",
    "    - $o_t = \\sigma(W_0[h_{t-1},x_t] + b_o)$\n",
    "    - $h_t = o_t\\cdot \\tanh{C_t}$\n",
    "    \n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-focus-o.png\" width=\"60%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zadatak:\n",
    "\n",
    "Po uzoru na Karpathyjev model RNN-a koji je naučen generirati stihove s obzirom na Shakespeare-ov korpus, napravite model koji će naučiti generirati stihove s obzirom na Cesarićev korpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import math\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pogledajmo srednju vrijednost broja svih riječi i broja paragrafa (pjesama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U prosjeku, Cesarićeva pjesma ima: 21.0 riječi u jednoj PJESMI.\n",
      "U prosjeku, Cesarićeva pjesma ima: 4.0 riječi u jednom STIHU.\n"
     ]
    }
   ],
   "source": [
    "with open('./cesaric/train.txt', 'r', encoding = 'utf-8') as outf:\n",
    "    paragraf = []\n",
    "    counter = 0\n",
    "    total_count_paragrafs = 0\n",
    "    sum_words = 0\n",
    "    for line in outf:\n",
    "        if line == '\\n': \n",
    "            total_count_paragrafs += 1\n",
    "            sum_words += len(paragraf)\n",
    "            paragraf = []\n",
    "        else:\n",
    "            if line.count('-') >= 3:\n",
    "                continue\n",
    "            else:\n",
    "                paragraf += re.findall(r\"[\\w]+\", line)\n",
    "        counter += 1\n",
    "            \n",
    "    print(\"U prosjeku, Cesarićeva pjesma ima: {} riječi u jednoj PJESMI.\".format(np.round(sum_words / total_count_paragrafs)))\n",
    "    print(\"U prosjeku, Cesarićeva pjesma ima: {} riječi u jednom STIHU.\".format(np.round(sum_words / counter)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dictionary(object):\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = []\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self.word2idx:\n",
    "            self.idx2word.append(word)\n",
    "            self.word2idx[word] = len(self.idx2word) - 1\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx2word)\n",
    "\n",
    "\n",
    "class Corpus(object):\n",
    "    def __init__(self, path):\n",
    "        self.dictionary = Dictionary()\n",
    "        self.train = self.tokenize(path)\n",
    "\n",
    "    def tokenize(self, path):\n",
    "        \n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            for line in f:\n",
    "                if line.count('-') >= 3:\n",
    "                    continue\n",
    "                else:\n",
    "                    words = re.findall(r\"[\\w']+|[.,!?;:]+\", line) + ['<eos>']\n",
    "                    for word in words:\n",
    "                        self.dictionary.add_word(word.lower())\n",
    "\n",
    "        # Tokenize file content\n",
    "        with open(path, 'r', encoding=\"utf8\") as f:\n",
    "            idss = []\n",
    "            for line in f:\n",
    "                if line.count('-') >= 3:\n",
    "                    continue\n",
    "                else:\n",
    "                    words = re.findall(r\"[\\w']+|[.,!?;:]+\", line) + ['<eos>']\n",
    "                    ids = []\n",
    "                    for word in words:\n",
    "                        ids.append(self.dictionary.word2idx[word.lower()])\n",
    "                    idss.append(torch.tensor(ids).type(torch.int64))\n",
    "            ids = torch.cat(idss)\n",
    "\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5, tie_weights=False):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.ntoken = ntoken\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.encoder = nn.Embedding(ntoken, ninp)\n",
    "        \n",
    "        if rnn_type in ['LSTM', 'GRU']:\n",
    "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        else:\n",
    "            try:\n",
    "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
    "            except KeyError:\n",
    "                raise ValueError( \"\"\"An invalid option for model was supplied,\n",
    "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")    \n",
    "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
    "            \n",
    "        self.decoder = nn.Linear(nhid, ntoken)\n",
    "\n",
    "        if tie_weights:\n",
    "            if nhid != ninp:\n",
    "                raise ValueError('When using the tied flag, nhid must be equal to emsize')\n",
    "            self.decoder.weight = self.encoder.weight\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        nn.init.uniform_(self.encoder.weight, -initrange, initrange)\n",
    "        nn.init.zeros_(self.decoder.weight)\n",
    "        nn.init.uniform_(self.decoder.weight, -initrange, initrange)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        emb = self.drop(self.encoder(input))\n",
    "        \n",
    "        output, hidden = self.rnn(emb, hidden)\n",
    "        output = self.drop(output)\n",
    "        \n",
    "        decoded = self.decoder(output)\n",
    "        decoded = decoded.view(-1, self.ntoken)\n",
    "        \n",
    "        return F.log_softmax(decoded, dim=1), hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters())\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (weight.new_zeros(self.nlayers, bsz, self.nhid),\n",
    "                    weight.new_zeros(self.nlayers, bsz, self.nhid))\n",
    "        else:\n",
    "            return weight.new_zeros(self.nlayers, bsz, self.nhid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support funkcije"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    # Work out how cleanly we can divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(source, i):\n",
    "    seq_len = min(SEQ_LEN, len(source) - 1 - i)\n",
    "    data = source[i:i + seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    if isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Konstante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'LSTM'\n",
    "\n",
    "EPOCHS = 200\n",
    "\n",
    "BATCH_SIZE = 21\n",
    "\n",
    "WORD_EMBEDDINGS = 200\n",
    "HIDDEN_UNITS_LAYER = 512\n",
    "N_LAYERS = 2\n",
    "\n",
    "SEQ_LEN = 21\n",
    "\n",
    "DROPOUT = 0.5\n",
    "\n",
    "LEARNING_RATE = 20\n",
    "\n",
    "SEED = datetime.now().microsecond\n",
    "\n",
    "WORDS = 500\n",
    "TEMPERATURE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Podaci za treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "corpus = Corpus('./cesaric/train.txt')\n",
    "train_data = batchify(corpus.train, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0,   40,    1,  ...,  209,    1,    7],\n",
       "        [   1,   17,  356,  ..., 2762,    7,  100],\n",
       "        [   2,  235,  327,  ..., 2763,  121,    1],\n",
       "        ...,\n",
       "        [   7,   13,  657,  ..., 2871,  128, 2321],\n",
       "        [  69,    7,  115,  ..., 2872, 2227,  205],\n",
       "        [ 234,  467,    7,  ..., 2873,    1,    7]])"
      ]
     },
     "execution_count": 524,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(corpus.dictionary)\n",
    "model = RNNModel(MODEL, ntokens, WORD_EMBEDDINGS, HIDDEN_UNITS_LAYER, N_LAYERS, DROPOUT, tie_weights=False)\n",
    "\n",
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treniranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LEARNING_RATE\n",
    "\n",
    "best_loss = None\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "        \n",
    "    model.train()\n",
    "    total_loss = 0.\n",
    "    \n",
    "    hidden = model.init_hidden(BATCH_SIZE)\n",
    "    ntokens = len(corpus.dictionary)\n",
    "    for _, i in enumerate(range(0, train_data.size(0) - 1, SEQ_LEN)):\n",
    "        data, targets = get_batch(train_data, i)\n",
    "\n",
    "        model.zero_grad()\n",
    "        \n",
    "        hidden = repackage_hidden(hidden)\n",
    "        output, hidden = model(data, hidden)\n",
    "        \n",
    "        loss = criterion(output, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        for p in model.parameters():\n",
    "            p.data.add_(p.grad, alpha=-learning_rate)\n",
    "\n",
    "        total_loss += len(data) * loss.item()\n",
    "    \n",
    "    loss = total_loss / (len(train_data) - 1)\n",
    "    \n",
    "    if not best_loss or loss < best_loss:\n",
    "        with open('./model/model.pt', 'wb') as f:\n",
    "            torch.save(model, f)\n",
    "        best_loss = loss\n",
    "    else:\n",
    "        learning_rate /= 4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generiranje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed: 602337\n",
      "Input: preneraženo\n"
     ]
    }
   ],
   "source": [
    "SEED = datetime.now().microsecond\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "print(\"Seed: {}\".format(SEED))\n",
    "\n",
    "with open('./model/model.pt', 'rb') as f:\n",
    "    model = torch.load(f)\n",
    "model.eval()\n",
    "\n",
    "corpus = Corpus('./cesaric/train.txt')\n",
    "ntokens = len(corpus.dictionary)\n",
    "\n",
    "hidden = model.init_hidden(1)\n",
    "\n",
    "input = torch.randint(ntokens, (1, 1), dtype=torch.long)\n",
    "\n",
    "print(\"Input: {}\".format(corpus.dictionary.idx2word[input]))\n",
    "\n",
    "with open('./output/generirani_cesaric.txt', 'w', encoding = 'utf-8') as outf:\n",
    "    with torch.no_grad(): \n",
    "        for i in range(WORDS):\n",
    "            output, hidden = model(input, hidden)\n",
    "            \n",
    "            word_weights = output.squeeze().div(TEMPERATURE).exp()\n",
    "            word_idx = torch.multinomial(word_weights, 1)[0]\n",
    "            input.fill_(word_idx)\n",
    "\n",
    "            word = corpus.dictionary.idx2word[word_idx]\n",
    "            \n",
    "            if word == '<eos>':\n",
    "                outf.write('\\n')\n",
    "            else:\n",
    "                if word in '.,!?;:':\n",
    "                    outf.write(word)\n",
    "                else:\n",
    "                    outf.write(' ' + word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Grubi opis:**  \n",
    "Model uzima teskt, tokenizira ga - svakoj rijeci pridodaje index, napravi batchove, pusti model da trenira na tim batchovima tako da gleda koja je vjerojatnost da ce se nakon i-te rijeci pojaviti bilo koja rijec iz tog batcha te \"uzima\" onu koja ima najvecu vjerojatnost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generirani tekst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " operete.\n",
      "\n",
      " ariju, koju zaboravi grad,\n",
      " zapjeva neko tu, u tišini\n",
      " svojega okna, u crveno veče,\n",
      " kao što pučanin puši u kini\n",
      " opijum, kada ga popuše svi,\n",
      " svi bogataši i mandarini.\n",
      "\n",
      " a žene rade, vječito rade,\n",
      " i rade i djecu doje,\n",
      " i rijetko i nerado u grad povedu\n",
      " dečke i kćerkice svoje,\n",
      " a kada kroz bogate ulice idu,\n",
      " one se izloga boje.\n",
      "\n",
      " jer tamo ima malih mašina\n",
      " i lutaka na hrpe,\n",
      " i sve je to lijepo, i sve je to ljepše\n",
      " no njihova lopta od krpe,\n",
      " i samo im želje na oči navru,\n",
      " i djeca i matere trpe.\n",
      "\n",
      " tramvaj je pred njima petnaest koraka,\n",
      " one se boje već prijeći.\n",
      " neko ih grubo na ulici gurne,\n",
      " one ne vele ni riječi\n",
      " davno su one već navikle na to\n",
      " da ih se gura i gnječi.\n",
      "\n",
      " maleno zvonce na vrat'ma dućana\n",
      " nije tek igračka puka.\n",
      " stupi l' u dućan, nenajavljen zvukom,\n",
      " prosjak il čovjek iz puka,\n",
      " mogla bi zgrabiti žemlju sa tezge\n",
      " kakva siromašna ruka.\n",
      "\n",
      " pred zrcalom, koje iskrivljuje lice,\n",
      " kosu po starinski dijele\n",
      " djevojke, koje na nezgrapne noge\n",
      " navlače čarape bijele\n",
      " jeftine čarape, čarape krasne,\n",
      " samo ako su cijele.\n",
      "\n",
      " a kada se koja ljepotica nađe,\n",
      " jedan se drugome prijete\n",
      " zaljubljenici iz cijeloga kraja\n",
      " derući za njom pete,\n",
      " a nje se nauživa mladić iz grada,\n",
      " i ostavi i nju, i dijete.\n",
      "\n",
      " ništa mi car svoju ulici\n",
      " zaboraviti teku\n",
      " na debelo blato kraj staroga plota\n",
      " i dvije, tri cigle na putu.\n",
      "\n",
      " svjetlucave i sasvim tanke niti\n",
      " lelujaju se zrakom amo tamo.\n",
      " bez ikakve su koristi te niti,\n",
      " a žao nam je da ih pokidamo.\n",
      "\n",
      " trenutak lebde, ko da i ne znadu,\n",
      " onako lake, za zemaljsku težu.\n",
      " tajanstveno su radosne i tihe.\n",
      " i ko bi znao s čim nas one vežu.\n",
      "\n",
      " onomu koji se digne iz groba\n",
      " dugo je hljeb u ustima gorak.\n",
      " on tiho hoda između ljudi,\n",
      " i nesiguran sad mu je korak.\n",
      "\n",
      " nerado njega sretaju ljudi.\n",
      " a koji ga ruke mu stišćući bodre\n",
      " potajno traže na njegovom licu\n",
      " pečate smrti sablasno modre.\n",
      "\n",
      " skidoše s njega mrtvačko platno.\n",
      " ali ne i s lica blijedoga sjenu.\n",
      " bezdanu tugu on ù sebi nosi\n",
      " i svoju smrt nedòvršenu.\n",
      "\n",
      " ne, ja se nisam oprostio s njom\n",
      " kad"
     ]
    }
   ],
   "source": [
    "with open('./output/generirani_cesaric.txt', 'r', encoding = 'utf-8') as outf:\n",
    "    for word in outf:\n",
    "        print(word, end = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Primjer\n",
    " * **Seed**: 892\n",
    " * **Input**: sudbini\n",
    " * **Temperature**: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "samo nam srce, samo toplo srce,  \n",
    "i sve je sreća što mi oči vide,  \n",
    "ati trenuci to su slavoluci  \n",
    "kroz koje ljubav u trijumfu ide!  \n",
    "  \n",
    "  \n",
    "ma kako uzdiglo se srce,  \n",
    "klonuti mora, mora pasti.  \n",
    "sudbino, prije no mi klone,  \n",
    "o daj mu još jedanput cvasti!  \n",
    "  \n",
    "  \n",
    "još jednom opij ga i digni  \n",
    "milinom jedne mlade žene,  \n",
    "još jedne zaljubljene oči  \n",
    "za ove oči zanesene.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Primjer\n",
    " * **Seed**: 18254\n",
    " * **Input**: kabina\n",
    " * **Temperature**: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i često, tkaju, tkaju  \n",
    "sve zanose nam žarke,  \n",
    "sve istine i varke.  \n",
    "\n",
    "\n",
    "nad svime bog je kronos:  \n",
    "ko livada! tužno ko skrilo.  \n",
    "podilazi se, otmi, pocigani!  \n",
    "\n",
    "\n",
    "gle iza moga stakla sjene:  \n",
    "nove muškarce, neznane žene.  \n",
    "  \n",
    "  \n",
    "probij se među ljude te  \n",
    "moderna čerga je coupé.  \n",
    "  \n",
    "  \n",
    "sve ti već uzeh prijatelje,  \n",
    "tek ti da ostaneš kod želje?  \n",
    "  \n",
    "\n",
    "uđi već jednom! što kolebaš?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Primjer\n",
    " * **Seed**: 425589\n",
    " * **Input**: talasom\n",
    " * **Temperature**: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ariju, koju zaboravi grad,  \n",
    "zapjeva neko tu, u tišini  \n",
    "svojega okna, u crveno veče,  \n",
    "kao što pučanin puši u kini  \n",
    "opijum, kada ga popuše svi,  \n",
    "svi bogataši i mandarini.  \n",
    "  \n",
    "  \n",
    "a žene rade, vječito rade,  \n",
    "i rade i djecu doje,  \n",
    "i rijetko i nerado u grad povedu  \n",
    "dečke i kćerkice svoje,  \n",
    "a kada kroz bogate ulice idu,  \n",
    "one se izloga boje.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Primjer\n",
    " * **Seed**: 675287\n",
    " * **Input**: podrhtava\n",
    " * **Temperature**: 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "da nisi bog, ni titan,  \n",
    "a kada je življa tu,  \n",
    "i svom se suncu zlatno smiješi,  \n",
    "dok te ne poždere dubina.  \n",
    "  \n",
    "  \n",
    "... i njena mala noga klecnu  \n",
    "na stubi vagona.  \n",
    "ona se iznenada lecnu:  \n",
    "pa što to radi ona?  \n",
    "  \n",
    "i još je mogla nogu povuć,  \n",
    "poslušavši klecaje,  \n",
    "i slomljena se doma dovuć,  \n",
    "i opet past u jecaje.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Primjer\n",
    " * **Seed**: 134619\n",
    " * **Input**: ruke\n",
    " * **Temperature**: 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "svjetalce jedno gori u daljini:  \n",
    "jedino svjetlo u noćnoj dolini:  \n",
    "i trepti nježno.  \n",
    "tako milo  \n",
    "ko da se smiješi.  \n",
    "ali gle! već se skrilo.  \n",
    "ugasilo se. i ko bi ga znao,  \n",
    "zašto ti bude odjedanput žao.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neki od početnih pokušaja"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daljini među vide stara  \n",
    "jesu voda svjetlost odjednom  \n",
    "svijet svoju njime jednoga  \n",
    "dopire neba čuje kuda  \n",
    "daje jutra tuga između  \n",
    "--  \n",
    "nestati dune to petrolejska ,   \n",
    "svlada prvo nizine il opojnost rad'je   \n",
    "rado ? , je car pobjednikom san on drugi ne crnom kao i sakriven pa   \n",
    "kosi . neznane je šuti zgrnu   \n",
    "ih zgnječenih dan budeći pomagali krevet  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
